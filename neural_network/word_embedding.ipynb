{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "irish-worry",
   "metadata": {},
   "source": [
    "# Hoax Detection Using Traditional Machine Learning\n",
    "## Dataset from Satria Data 2020 - Big Data Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-portuguese",
   "metadata": {},
   "source": [
    "This method represents words as dense word vectors which are trained unlike the one-hot encoding which are hardcoded. This means that the word embeddings collect more information into fewer dimensions. **Word embeddings do not understand the text as a human would, but they rather map the statistical structure of the language used in the corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-annotation",
   "metadata": {},
   "source": [
    "## Word Embedding Using Keras Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "black-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from string import punctuation\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, GlobalMaxPooling1D\n",
    "from pandarallel import pandarallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offensive-cloud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing Initialization\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lyric-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "train_data = pd.read_excel(\"../Dataset/training/DataLatih.xlsx\", engine=\"openpyxl\")\n",
    "test_data = pd.read_excel(\"../Dataset/testing/DataUji.xlsx\", engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sensitive-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct train dataframe\n",
    "train_df = pd.DataFrame()\n",
    "train_df[\"konten\"] = train_data[\"judul_translate\"] + \" \" + train_data[\"narasi_translate\"]\n",
    "train_df[\"Class\"] = train_data[\"label\"]\n",
    "\n",
    "# Reconstruct test dataframe\n",
    "test_df = pd.DataFrame()\n",
    "test_df[\"ID\"] = test_data[\"ID\"]\n",
    "test_df[\"konten\"] = test_data[\"judul_translate\"] + \" \" + test_data[\"narasi_translate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fleet-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopword list, indonesia\n",
    "STOPWORDS = set(StopWordRemoverFactory().get_stop_words() + stopwords.words('english'))\n",
    "\n",
    "# define list kata singkat\n",
    "KATASINGKAT = {\"dlm\":\"dalam\", \"gw\":\"saya\", \"yg\":\"yang\", \"lu\":\"kamu\", \"dkt\":\"dekat\", \"kalo\":\"kalau\", \"n\":\"and\"}\n",
    "\n",
    "# define stemmer sastrawi for Indonesia\n",
    "stemmer_ind = StemmerFactory().create_stemmer()\n",
    "stemmer_eng = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equal-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of preprocessing\n",
    "def remove_kata_singkat(word):\n",
    "    if word in list(KATASINGKAT.keys()):\n",
    "        return KATASINGKAT.get(word)\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def normalize_word(row):\n",
    "    # remove punctuation\n",
    "    konten = re.sub(r'[^a-zA-Z\\s]', ' ', row.konten, re.I|re.A)\n",
    "    \n",
    "    # case folding and remove kata singkat\n",
    "    konten = \" \".join([remove_kata_singkat(word.lower()).strip() for word in nltk.word_tokenize(konten)])\n",
    "    \n",
    "    # remove stopword and number\n",
    "    konten = \" \".join([word for word in nltk.word_tokenize(konten) if word not in punctuation and word.isalpha() and word not in STOPWORDS])\n",
    "    \n",
    "    # stemming\n",
    "    konten = stemmer_ind.stem(konten)\n",
    "    konten = stemmer_eng.stem(konten)\n",
    "    \n",
    "    # final assignment\n",
    "    row.konten = konten\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personal-highway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdd2bf3084e4092a80f2e0981ce96fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1058), Label(value='0 / 1058'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c166149e7ff427e88a2f60af45e7256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=118), Label(value='0 / 118'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parallel preprocess to dataframe with progressbar\n",
    "train_df = train_df.parallel_apply(normalize_word, axis=1)\n",
    "test_df = test_df.parallel_apply(normalize_word, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "steady-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "konten_train = train_df[\"konten\"]\n",
    "konten_test = test_df[\"konten\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "golden-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# konten_all = konten_train.append(konten_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "naval-turkish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4231,), (470,))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "konten_train.shape, konten_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "reported-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate object\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "stock-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(konten_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "auburn-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(konten_train)\n",
    "y = train_df[\"Class\"]\n",
    "X_test = tokenizer.texts_to_sequences(konten_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "alleged-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "featured-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "monthly-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fb317758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14027"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "hungarian-appointment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 500, 50)           701350    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 711,551\n",
      "Trainable params: 711,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DEFINE MODEL\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "informal-polymer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 2s 9ms/step - loss: 0.5440 - accuracy: 0.7938 - val_loss: 0.4809 - val_accuracy: 0.8111\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 1s 7ms/step - loss: 0.4320 - accuracy: 0.8219 - val_loss: 0.4469 - val_accuracy: 0.8099\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 1s 7ms/step - loss: 0.3220 - accuracy: 0.8595 - val_loss: 0.5151 - val_accuracy: 0.8276\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 1s 7ms/step - loss: 0.1821 - accuracy: 0.9333 - val_loss: 0.6095 - val_accuracy: 0.8052\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 1s 7ms/step - loss: 0.0641 - accuracy: 0.9823 - val_loss: 0.7078 - val_accuracy: 0.7603\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 1s 7ms/step - loss: 0.0225 - accuracy: 0.9961 - val_loss: 0.8462 - val_accuracy: 0.7757\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 1s 7ms/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 0.9522 - val_accuracy: 0.7863\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 1.0215 - val_accuracy: 0.7863\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 1.0618 - val_accuracy: 0.7745\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 1s 7ms/step - loss: 8.4321e-04 - accuracy: 1.0000 - val_loss: 1.1126 - val_accuracy: 0.7769\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "forty-praise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9994\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "answering-slope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7910\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "be619204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 500), dtype=tf.float32, name='embedding_8_input'), name='embedding_8_input', description=\"created by layer 'embedding_8_input'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7199089 ],\n",
       "       [0.12223333],\n",
       "       [0.5589266 ],\n",
       "       [0.8440214 ],\n",
       "       [0.9654064 ],\n",
       "       [0.3387453 ],\n",
       "       [0.5810466 ],\n",
       "       [0.1708524 ],\n",
       "       [0.9654064 ],\n",
       "       [0.7340007 ],\n",
       "       [0.618966  ],\n",
       "       [0.12223333],\n",
       "       [0.14833039],\n",
       "       [0.08844292],\n",
       "       [0.9654064 ],\n",
       "       [0.8132427 ],\n",
       "       [0.95487833],\n",
       "       [0.3387453 ],\n",
       "       [0.5810466 ],\n",
       "       [0.1708524 ],\n",
       "       [0.9654064 ],\n",
       "       [0.7340007 ],\n",
       "       [0.15317488]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-luxembourg",
   "metadata": {},
   "source": [
    "## Using Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-copying",
   "metadata": {},
   "source": [
    "An alternative is to use a precomputed embedding space that utilizes a much larger corpus. It is possible to precompute word embeddings by simply training them on a large corpus of text. Among the most popular methods are [Word2Vec](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) developed by Google and [GloVe](https://nlp.stanford.edu/projects/glove/) (Global Vectors for Word Representation) developed by the Stanford NLP Group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-skating",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://realpython.com/python-keras-text-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
